coredns:
  isClusterService: false
  serviceType: LoadBalancer
  service:
    annotations:
      # Kann nicht auf selbe IP wie Ingress assigned werden von Kube-VIP.
      # Wenn hier die gleiche IP angegeben wird passiert folgendes:
      # Ein Node bekommt das Lease im namespace ingress-nginx/kubevip-ingress-public-ingress-nginx-controller
      # aber ein anderer Node bekommt das Lease networking/kubevip-networking-coredns
      # und wieder ein anderer bekommt das Lease gitea/kubevip-gitea-ssh
      kube-vip.io/loadbalancerIPs: 192.168.2.92

  env:
    - name: ETCD_USERNAME
      value: root
    - name: ETCD_PASSWORD
      valueFrom:
        secretKeyRef:
          key: etcd-root-password
          name: etcd-password

  # Default zone is what Kubernetes recommends:
  # https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#coredns-configmap-options
  servers:
    - zones:
        - zone: .
      port: 53
      # -- expose the service on a different port
      # servicePort: 5353
      # If serviceType is nodePort you can specify nodePort here
      # nodePort: 30053
      # hostPort: 53
      plugins:
        - name: errors
        # Serves a /health endpoint on :8080, required for livenessProbe
        - name: health
          configBlock: |-
            lameduck 5s
        # Serves a /ready endpoint on :8181, required for readinessProbe
        - name: ready
        # Serves a /metrics endpoint on :9153, required for serviceMonitor
        - name: prometheus
          parameters: 0.0.0.0:9153

        - name: cache
          parameters: 30
        - name: loop
        - name: reload
        - name: loadbalance
        # Limit to local hostnames and don't fall through for arpa domains. This should prevent grabbing public dns entries.
        # But allow fallthrough for public domains (so that *.mon. domains can be reached)
        - name: etcd
          parameters: home.arpa felix-prasse.de
          configBlock: |-
            fallthrough felix-prasse.de
            path /skydns
            endpoint http://10.43.142.0:2379
            credentials {$ETCD_USERNAME} {$ETCD_PASSWORD}
        - name: forward
          parameters: felix-prasse.de 1.1.1.1
        - name: forward
          parameters: . /etc/resolv.conf
etcd:
  # TODO Migrate to https://etcd.io/docs/v3.5/op-guide/kubernetes/
  image:
    repository: docker.io/bitnamilegacy
  persistence:
    enabled: false
    storageClass: rook-local
  # This job fails if the networking-etcd-jwt-token secret is not there yet?!
  preUpgradeJob:
    enabled: false

  replicaCount: 1
#  resources:
#    requests:
#      cpu: 10m
#      memory: 100Mi
#    limits:
#      cpu: 100m
#      memory: 250Mi

  auth:
    rbac:
      existingSecret: etcd-password
      existingSecretPasswordKey: etcd-root-password
    token:
      enabled: false

  service:
    clusterIP: 10.43.142.0
  podAntiAffinityPreset: hard
